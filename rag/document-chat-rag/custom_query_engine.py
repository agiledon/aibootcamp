"""
è‡ªå®šä¹‰æŸ¥è¯¢å¼•æ“æ¨¡å—
æä¾›åŸºäºæ–‡æ¡£è¿‡æ»¤çš„æŸ¥è¯¢å¼•æ“å®ç°
"""

import logging
from typing import List, Optional, Any
from llama_index.core.query_engine import BaseQueryEngine
from llama_index.core.schema import QueryBundle, NodeWithScore
from llama_index.core.indices import VectorStoreIndex
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.postprocessor.types import BaseNodePostprocessor
from llama_index.core.callbacks import CallbackManager
from pydantic import Field

logger = logging.getLogger(__name__)


class FileFilterPostprocessor(BaseNodePostprocessor):
    """åŸºäºæ–‡ä»¶åè¿‡æ»¤çš„åå¤„ç†å™¨"""
    
    target_files: Optional[List[str]] = Field(default=None, description="ç›®æ ‡æ–‡ä»¶ååˆ—è¡¨")
    
    def __init__(self, target_files: Optional[List[str]] = None, **kwargs):
        """
        åˆå§‹åŒ–æ–‡ä»¶è¿‡æ»¤åå¤„ç†å™¨
        
        Args:
            target_files: ç›®æ ‡æ–‡ä»¶ååˆ—è¡¨
        """
        super().__init__(target_files=target_files, **kwargs)
        if target_files:
            self.target_files = target_files
        logger.info(f"åˆå§‹åŒ–æ–‡ä»¶è¿‡æ»¤åå¤„ç†å™¨ï¼Œç›®æ ‡æ–‡ä»¶: {self.target_files}")
    
    def _postprocess_nodes(
        self, 
        nodes: List[NodeWithScore], 
        query_bundle: Optional[QueryBundle] = None
    ) -> List[NodeWithScore]:
        """
        è¿‡æ»¤èŠ‚ç‚¹ï¼Œåªä¿ç•™ç›®æ ‡æ–‡ä»¶ä¸­çš„èŠ‚ç‚¹
        
        Args:
            nodes: å¾…è¿‡æ»¤çš„èŠ‚ç‚¹åˆ—è¡¨
            query_bundle: æŸ¥è¯¢åŒ…ï¼ˆæœªä½¿ç”¨ï¼‰
            
        Returns:
            è¿‡æ»¤åçš„èŠ‚ç‚¹åˆ—è¡¨
        """
        if not self.target_files:
            logger.info("æ— ç›®æ ‡æ–‡ä»¶é™åˆ¶ï¼Œè¿”å›æ‰€æœ‰èŠ‚ç‚¹")
            return nodes
        
        target_files_set = set(self.target_files)
        filtered_nodes = []
        for node in nodes:
            if hasattr(node, 'node') and hasattr(node.node, 'metadata'):
                file_name = node.node.metadata.get('file_name', '')
                if file_name in target_files_set:
                    filtered_nodes.append(node)
                    logger.debug(f"ä¿ç•™èŠ‚ç‚¹ï¼Œæ–‡ä»¶: {file_name}")
                else:
                    logger.debug(f"è¿‡æ»¤èŠ‚ç‚¹ï¼Œæ–‡ä»¶: {file_name}")
            else:
                # å¦‚æœæ²¡æœ‰å…ƒæ•°æ®ï¼Œä¹Ÿä¿ç•™èŠ‚ç‚¹ï¼ˆå¯èƒ½æ˜¯ç³»ç»ŸèŠ‚ç‚¹ï¼‰
                filtered_nodes.append(node)
                logger.debug("ä¿ç•™æ— å…ƒæ•°æ®èŠ‚ç‚¹")
        
        logger.info(f"èŠ‚ç‚¹è¿‡æ»¤å®Œæˆï¼ŒåŸå§‹èŠ‚ç‚¹æ•°: {len(nodes)}, è¿‡æ»¤åèŠ‚ç‚¹æ•°: {len(filtered_nodes)}")
        return filtered_nodes


class FilteredQueryEngine(BaseQueryEngine):
    """è‡ªå®šä¹‰æŸ¥è¯¢å¼•æ“ï¼Œæ”¯æŒæ–‡æ¡£è¿‡æ»¤"""
    
    def __init__(
        self,
        index: VectorStoreIndex,
        target_files: Optional[List[str]] = None,
        similarity_top_k: int = 5,
        streaming: bool = True,
        llm: Optional[Any] = None,
        callback_manager: Optional[CallbackManager] = None
    ):
        """
        åˆå§‹åŒ–è¿‡æ»¤æŸ¥è¯¢å¼•æ“
        
        Args:
            index: å‘é‡å­˜å‚¨ç´¢å¼•
            target_files: ç›®æ ‡æ–‡ä»¶ååˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºå…¨çŸ¥è¯†åº“
            similarity_top_k: ç›¸ä¼¼åº¦æ£€ç´¢çš„top-kæ•°é‡
            streaming: æ˜¯å¦å¯ç”¨æµå¼å“åº”
            llm: è¯­è¨€æ¨¡å‹å®ä¾‹
            callback_manager: å›è°ƒç®¡ç†å™¨
        """
        # å¦‚æœæ²¡æœ‰æä¾›å›è°ƒç®¡ç†å™¨ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„ï¼Œé¿å…å›è°ƒæ ˆçŠ¶æ€é—®é¢˜
        if callback_manager is None:
            from llama_index.core.callbacks import CallbackManager
            callback_manager = CallbackManager()
        
        super().__init__(callback_manager=callback_manager)
        self.index = index
        self.target_files = target_files
        self.similarity_top_k = similarity_top_k
        self.streaming = streaming
        self.llm = llm
        
        # åˆ›å»ºåŸºç¡€æŸ¥è¯¢å¼•æ“
        self._base_query_engine = None
        self._create_base_query_engine()
        
        logger.info(f"åˆå§‹åŒ–è¿‡æ»¤æŸ¥è¯¢å¼•æ“ï¼Œç›®æ ‡æ–‡ä»¶: {self.target_files}, top_k: {self.similarity_top_k}")
    
    def query(self, query_str: str):
        """
        åŒæ­¥æŸ¥è¯¢æ–¹æ³•
        
        Args:
            query_str: æŸ¥è¯¢å­—ç¬¦ä¸²
            
        Returns:
            æŸ¥è¯¢å“åº”
        """
        from llama_index.core.schema import QueryBundle
        query_bundle = QueryBundle(query_str)
        return self._query(query_bundle)
    
    def _create_base_query_engine(self):
        """åˆ›å»ºåŸºç¡€æŸ¥è¯¢å¼•æ“"""
        try:
            # åˆ›å»ºåå¤„ç†å™¨åˆ—è¡¨
            postprocessors = []
            
            # å¦‚æœæœ‰ç›®æ ‡æ–‡ä»¶ï¼Œæ·»åŠ æ–‡ä»¶è¿‡æ»¤åå¤„ç†å™¨
            if self.target_files:
                file_filter = FileFilterPostprocessor(self.target_files)
                postprocessors.append(file_filter)
                logger.info(f"æ·»åŠ æ–‡ä»¶è¿‡æ»¤åå¤„ç†å™¨ï¼Œç›®æ ‡æ–‡ä»¶: {self.target_files}")
            
            # åˆ›å»ºæŸ¥è¯¢å¼•æ“ï¼ˆä¸ä¼ é€’ callback_managerï¼Œé¿å…é‡å¤å‚æ•°é”™è¯¯ï¼‰
            if self.llm:
                self._base_query_engine = self.index.as_query_engine(
                    similarity_top_k=self.similarity_top_k,
                    node_postprocessors=postprocessors,
                    streaming=self.streaming,
                    llm=self.llm
                )
            else:
                self._base_query_engine = self.index.as_query_engine(
                    similarity_top_k=self.similarity_top_k,
                    node_postprocessors=postprocessors,
                    streaming=self.streaming
                )
            
            logger.info("âœ… åŸºç¡€æŸ¥è¯¢å¼•æ“åˆ›å»ºæˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºåŸºç¡€æŸ¥è¯¢å¼•æ“å¤±è´¥: {e}")
            raise e
    
    def _query(self, query_bundle: QueryBundle):
        """
        æ‰§è¡ŒæŸ¥è¯¢
        
        Args:
            query_bundle: æŸ¥è¯¢åŒ…
            
        Returns:
            æŸ¥è¯¢å“åº”
        """
        try:
            logger.info(f"ğŸ” æ‰§è¡ŒæŸ¥è¯¢: {query_bundle.query_str[:50]}...")
            
            if self._base_query_engine is None:
                raise RuntimeError("åŸºç¡€æŸ¥è¯¢å¼•æ“æœªåˆå§‹åŒ–")
            
            # æ‰§è¡ŒæŸ¥è¯¢ï¼ˆè¿‡æ»¤ç”±åå¤„ç†å™¨è‡ªåŠ¨å¤„ç†ï¼‰
            response = self._base_query_engine._query(query_bundle)
            
            logger.info("âœ… æŸ¥è¯¢æ‰§è¡ŒæˆåŠŸ")
            return response
            
        except IndexError as e:
            if "pop from empty list" in str(e):
                logger.warning(f"âš ï¸ æ£€æµ‹åˆ°å›è°ƒç®¡ç†å™¨æ ˆçŠ¶æ€é—®é¢˜ï¼Œå°è¯•é‡ç½®å›è°ƒç®¡ç†å™¨: {e}")
                # é‡ç½®å›è°ƒç®¡ç†å™¨çŠ¶æ€
                try:
                    from llama_index.core.callbacks import CallbackManager
                    self.callback_manager = CallbackManager()
                    # é‡æ–°åˆ›å»ºåŸºç¡€æŸ¥è¯¢å¼•æ“
                    self._create_base_query_engine()
                    # é‡è¯•æŸ¥è¯¢
                    response = self._base_query_engine._query(query_bundle)
                    logger.info("âœ… æŸ¥è¯¢æ‰§è¡ŒæˆåŠŸï¼ˆé‡è¯•åï¼‰")
                    return response
                except Exception as retry_e:
                    logger.error(f"âŒ é‡è¯•æŸ¥è¯¢å¤±è´¥: {retry_e}")
                    raise e
            else:
                logger.error(f"âŒ æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}")
                raise e
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}")
            raise e
    
    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:
        """
        æ£€ç´¢ç›¸å…³èŠ‚ç‚¹
        
        Args:
            query_bundle: æŸ¥è¯¢åŒ…
            
        Returns:
            æ£€ç´¢åˆ°çš„èŠ‚ç‚¹åˆ—è¡¨
        """
        try:
            logger.info(f"ğŸ” æ£€ç´¢èŠ‚ç‚¹: {query_bundle.query_str[:50]}...")
            
            if self._base_query_engine is None:
                raise RuntimeError("åŸºç¡€æŸ¥è¯¢å¼•æ“æœªåˆå§‹åŒ–")
            
            nodes = self._base_query_engine._retrieve(query_bundle)
            
            logger.info(f"âœ… æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(nodes)} ä¸ªèŠ‚ç‚¹")
            return nodes
            
        except Exception as e:
            logger.error(f"âŒ èŠ‚ç‚¹æ£€ç´¢å¤±è´¥: {e}")
            raise e
    
    def get_target_files(self) -> Optional[List[str]]:
        """è·å–ç›®æ ‡æ–‡ä»¶åˆ—è¡¨"""
        return self.target_files
    
    def set_target_files(self, target_files: Optional[List[str]]):
        """è®¾ç½®ç›®æ ‡æ–‡ä»¶åˆ—è¡¨"""
        self.target_files = target_files
        logger.info(f"æ›´æ–°ç›®æ ‡æ–‡ä»¶åˆ—è¡¨: {self.target_files}")
        
        # é‡æ–°åˆ›å»ºåŸºç¡€æŸ¥è¯¢å¼•æ“ä»¥åº”ç”¨æ–°çš„è¿‡æ»¤æ¡ä»¶
        self._create_base_query_engine()
    
    async def _aquery(self, query_bundle: QueryBundle):
        """
        å¼‚æ­¥æŸ¥è¯¢æ–¹æ³•
        
        Args:
            query_bundle: æŸ¥è¯¢åŒ…
            
        Returns:
            æŸ¥è¯¢å“åº”
        """
        # å¯¹äºæˆ‘ä»¬çš„ç”¨ä¾‹ï¼Œç›´æ¥è°ƒç”¨åŒæ­¥æ–¹æ³•
        return self._query(query_bundle)
    
    def _get_prompt_modules(self):
        """
        è·å–æç¤ºæ¨¡å—
        
        Returns:
            æç¤ºæ¨¡å—å­—å…¸
        """
        return {}
