"""
è‡ªå®šä¹‰æŸ¥è¯¢å¼•æ“æ¨¡å—
æä¾›åŸºäºæ–‡æ¡£è¿‡æ»¤çš„æŸ¥è¯¢å¼•æ“å®ç°
"""

import logging
from typing import List, Optional, Any
from llama_index.core.query_engine import BaseQueryEngine
from llama_index.core.schema import QueryBundle, NodeWithScore
from llama_index.core.indices import VectorStoreIndex
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.postprocessor.types import BaseNodePostprocessor
from llama_index.core.callbacks import CallbackManager
from pydantic import Field

logger = logging.getLogger(__name__)


class FileFilterPostprocessor(BaseNodePostprocessor):
    """åŸºäºæ–‡ä»¶åè¿‡æ»¤çš„åå¤„ç†å™¨"""
    
    target_files: Optional[List[str]] = Field(default=None, description="ç›®æ ‡æ–‡ä»¶ååˆ—è¡¨")
    
    def __init__(self, target_files: Optional[List[str]] = None, **kwargs):
        """
        åˆå§‹åŒ–æ–‡ä»¶è¿‡æ»¤åå¤„ç†å™¨
        
        Args:
            target_files: ç›®æ ‡æ–‡ä»¶ååˆ—è¡¨
        """
        super().__init__(target_files=target_files, **kwargs)
        if target_files:
            self.target_files = target_files
        logger.info(f"åˆå§‹åŒ–æ–‡ä»¶è¿‡æ»¤åå¤„ç†å™¨ï¼Œç›®æ ‡æ–‡ä»¶: {self.target_files}")
    
    def _postprocess_nodes(
        self, 
        nodes: List[NodeWithScore], 
        query_bundle: Optional[QueryBundle] = None
    ) -> List[NodeWithScore]:
        """
        è¿‡æ»¤èŠ‚ç‚¹ï¼Œåªä¿ç•™ç›®æ ‡æ–‡ä»¶ä¸­çš„èŠ‚ç‚¹
        
        Args:
            nodes: å¾…è¿‡æ»¤çš„èŠ‚ç‚¹åˆ—è¡¨
            query_bundle: æŸ¥è¯¢åŒ…ï¼ˆæœªä½¿ç”¨ï¼‰
            
        Returns:
            è¿‡æ»¤åçš„èŠ‚ç‚¹åˆ—è¡¨
        """
        if not self.target_files:
            logger.info("æ— ç›®æ ‡æ–‡ä»¶é™åˆ¶ï¼Œè¿”å›æ‰€æœ‰èŠ‚ç‚¹")
            return nodes
        
        target_files_set = set(self.target_files)
        filtered_nodes = []
        for node in nodes:
            if hasattr(node, 'node') and hasattr(node.node, 'metadata'):
                file_name = node.node.metadata.get('file_name', '')
                if file_name in target_files_set:
                    filtered_nodes.append(node)
                    logger.debug(f"ä¿ç•™èŠ‚ç‚¹ï¼Œæ–‡ä»¶: {file_name}")
                else:
                    logger.debug(f"è¿‡æ»¤èŠ‚ç‚¹ï¼Œæ–‡ä»¶: {file_name}")
            else:
                # å¦‚æœæ²¡æœ‰å…ƒæ•°æ®ï¼Œä¹Ÿä¿ç•™èŠ‚ç‚¹ï¼ˆå¯èƒ½æ˜¯ç³»ç»ŸèŠ‚ç‚¹ï¼‰
                filtered_nodes.append(node)
                logger.debug("ä¿ç•™æ— å…ƒæ•°æ®èŠ‚ç‚¹")
        
        logger.info(f"èŠ‚ç‚¹è¿‡æ»¤å®Œæˆï¼ŒåŸå§‹èŠ‚ç‚¹æ•°: {len(nodes)}, è¿‡æ»¤åèŠ‚ç‚¹æ•°: {len(filtered_nodes)}")
        return filtered_nodes


class FilteredQueryEngine(BaseQueryEngine):
    """è‡ªå®šä¹‰æŸ¥è¯¢å¼•æ“ï¼Œæ”¯æŒæ–‡æ¡£è¿‡æ»¤"""
    
    def __init__(
        self,
        index: VectorStoreIndex,
        target_files: Optional[List[str]] = None,
        similarity_top_k: int = 5,
        streaming: bool = True,
        llm: Optional[Any] = None,
        callback_manager: Optional[CallbackManager] = None
    ):
        """
        åˆå§‹åŒ–è¿‡æ»¤æŸ¥è¯¢å¼•æ“
        
        Args:
            index: å‘é‡å­˜å‚¨ç´¢å¼•
            target_files: ç›®æ ‡æ–‡ä»¶ååˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºå…¨çŸ¥è¯†åº“
            similarity_top_k: ç›¸ä¼¼åº¦æ£€ç´¢çš„top-kæ•°é‡
            streaming: æ˜¯å¦å¯ç”¨æµå¼å“åº”
            llm: è¯­è¨€æ¨¡å‹å®ä¾‹
            callback_manager: å›è°ƒç®¡ç†å™¨
        """
        # å¦‚æœæ²¡æœ‰æä¾›å›è°ƒç®¡ç†å™¨ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„ï¼Œé¿å…å›è°ƒæ ˆçŠ¶æ€é—®é¢˜
        if callback_manager is None:
            from llama_index.core.callbacks import CallbackManager
            callback_manager = CallbackManager()
        
        super().__init__(callback_manager=callback_manager)
        self.index = index
        self.target_files = target_files
        self.similarity_top_k = similarity_top_k
        self.streaming = streaming
        self.llm = llm
        
        # åˆ›å»ºåŸºç¡€æŸ¥è¯¢å¼•æ“
        self._base_query_engine = None
        self._create_base_query_engine()
        
        logger.info(f"åˆå§‹åŒ–è¿‡æ»¤æŸ¥è¯¢å¼•æ“ï¼Œç›®æ ‡æ–‡ä»¶: {self.target_files}, top_k: {self.similarity_top_k}")
    
    def query(self, query_str: str):
        """
        åŒæ­¥æŸ¥è¯¢æ–¹æ³•
        
        Args:
            query_str: æŸ¥è¯¢å­—ç¬¦ä¸²
            
        Returns:
            æŸ¥è¯¢å“åº”
        """
        from llama_index.core.schema import QueryBundle
        query_bundle = QueryBundle(query_str)
        return self._query(query_bundle)
    
    def _create_base_query_engine(self):
        """åˆ›å»ºåŸºç¡€æŸ¥è¯¢å¼•æ“"""
        try:
            # åˆ›å»ºåå¤„ç†å™¨åˆ—è¡¨
            postprocessors = []
            
            # å¦‚æœæœ‰ç›®æ ‡æ–‡ä»¶ï¼Œæ·»åŠ æ–‡ä»¶è¿‡æ»¤åå¤„ç†å™¨
            if self.target_files:
                file_filter = FileFilterPostprocessor(self.target_files)
                postprocessors.append(file_filter)
                logger.info(f"æ·»åŠ æ–‡ä»¶è¿‡æ»¤åå¤„ç†å™¨ï¼Œç›®æ ‡æ–‡ä»¶: {self.target_files}")
            
            # åˆ›å»ºæŸ¥è¯¢å¼•æ“ï¼ˆä¸ä¼ é€’ callback_managerï¼Œé¿å…é‡å¤å‚æ•°é”™è¯¯ï¼‰
            if self.llm:
                self._base_query_engine = self.index.as_query_engine(
                    similarity_top_k=self.similarity_top_k,
                    node_postprocessors=postprocessors,
                    streaming=self.streaming,
                    llm=self.llm
                )
            else:
                self._base_query_engine = self.index.as_query_engine(
                    similarity_top_k=self.similarity_top_k,
                    node_postprocessors=postprocessors,
                    streaming=self.streaming
                )
            
            logger.info("âœ… åŸºç¡€æŸ¥è¯¢å¼•æ“åˆ›å»ºæˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºåŸºç¡€æŸ¥è¯¢å¼•æ“å¤±è´¥: {e}")
            raise e
    
    def _query(self, query_bundle: QueryBundle):
        """
        æ‰§è¡ŒæŸ¥è¯¢
        
        Args:
            query_bundle: æŸ¥è¯¢åŒ…
            
        Returns:
            æŸ¥è¯¢å“åº”
        """
        try:
            logger.info(f"ğŸ” æ‰§è¡ŒæŸ¥è¯¢: {query_bundle.query_str[:50]}...")
            
            if self._base_query_engine is None:
                raise RuntimeError("åŸºç¡€æŸ¥è¯¢å¼•æ“æœªåˆå§‹åŒ–")
            
            # å¦‚æœæœ‰ç›®æ ‡æ–‡ä»¶è¿‡æ»¤éœ€æ±‚ï¼Œæ‰‹åŠ¨åº”ç”¨è¿‡æ»¤
            if self.target_files:
                logger.info(f"ğŸ” æ‰‹åŠ¨åº”ç”¨æ–‡ä»¶è¿‡æ»¤ï¼Œç›®æ ‡æ–‡ä»¶: {self.target_files}")
                
                # å…ˆå°è¯•ä»ç›®æ ‡æ–‡ä»¶ä¸­æ£€ç´¢æ›´å¤šèŠ‚ç‚¹
                target_nodes = self._retrieve_from_target_files(query_bundle)
                logger.info(f"ğŸ” ä»ç›®æ ‡æ–‡ä»¶æ£€ç´¢åˆ° {len(target_nodes)} ä¸ªèŠ‚ç‚¹")
                
                # å¦‚æœç›®æ ‡æ–‡ä»¶èŠ‚ç‚¹ä¸å¤Ÿï¼Œå°è¯•å…³é”®è¯æ£€ç´¢
                if len(target_nodes) < self.similarity_top_k:
                    logger.info(f"ğŸ” ç›®æ ‡æ–‡ä»¶èŠ‚ç‚¹ä¸è¶³ï¼Œå°è¯•å…³é”®è¯æ£€ç´¢")
                    
                    # ä»ç›®æ ‡æ–‡ä»¶åä¸­æå–å…³é”®è¯
                    keywords = self._extract_keywords_from_target_files()
                    logger.info(f"ğŸ” æå–çš„å…³é”®è¯: {keywords}")
                    
                    # å°è¯•ä½¿ç”¨å…³é”®è¯æ£€ç´¢
                    keyword_nodes = []
                    for keyword in keywords:
                        if keyword and len(keyword) > 1:  # è¿‡æ»¤æ‰å¤ªçŸ­çš„è¯
                            logger.info(f"ğŸ” å°è¯•å…³é”®è¯æ£€ç´¢: '{keyword}'")
                            keyword_query = QueryBundle(keyword)
                            nodes = self._retrieve(keyword_query)
                            filtered_nodes = self._apply_file_filter(nodes)
                            keyword_nodes.extend(filtered_nodes)
                            logger.info(f"ğŸ” å…³é”®è¯ '{keyword}' æ‰¾åˆ° {len(filtered_nodes)} ä¸ªç›®æ ‡æ–‡ä»¶èŠ‚ç‚¹")
                    
                    # åˆå¹¶å¹¶å»é‡
                    all_target_nodes = self._merge_nodes(target_nodes, keyword_nodes)
                    
                    # å¦‚æœè¿˜æ˜¯æ²¡æœ‰è¶³å¤Ÿçš„èŠ‚ç‚¹ï¼Œè¿›è¡Œå…¨å±€æ£€ç´¢
                    if len(all_target_nodes) < self.similarity_top_k:
                        logger.info(f"ğŸ” å…³é”®è¯æ£€ç´¢åä»ä¸è¶³ï¼Œè¿›è¡Œå…¨å±€æ£€ç´¢è¡¥å……")
                        global_nodes = self._retrieve(query_bundle)
                        filtered_global_nodes = self._apply_file_filter(global_nodes)
                        all_target_nodes = self._merge_nodes(all_target_nodes, filtered_global_nodes)
                else:
                    all_target_nodes = target_nodes
                
                logger.info(f"ğŸ” æœ€ç»ˆç›®æ ‡æ–‡ä»¶èŠ‚ç‚¹æ•°: {len(all_target_nodes)}")
                
                if not all_target_nodes:
                    logger.warning("âš ï¸ ç›®æ ‡æ–‡ä»¶ä¸­æ²¡æœ‰ç›¸å…³èŠ‚ç‚¹ï¼Œè¿”å›ç©ºå“åº”")
                    # è¿”å›ä¸€ä¸ªç©ºçš„å“åº”
                    from llama_index.core import Response
                    return Response(response="æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæˆ‘æ— æ³•æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚", source_nodes=[])
                
                # æ‰‹åŠ¨æ„å»ºå“åº”
                from llama_index.core import Response
                response = Response(
                    response="",  # è¿™é‡Œéœ€è¦è°ƒç”¨ LLM ç”Ÿæˆå›ç­”
                    source_nodes=all_target_nodes
                )
                
                # å¦‚æœæœ‰ LLMï¼Œç”Ÿæˆå›ç­”
                if self.llm:
                    # æ„å»ºä¸Šä¸‹æ–‡
                    context_str = "\n\n".join([node.node.text for node in all_target_nodes])
                    prompt = f"åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ï¼š\n\nä¸Šä¸‹æ–‡ï¼š\n{context_str}\n\né—®é¢˜ï¼š{query_bundle.query_str}\n\nå›ç­”ï¼š"
                    
                    llm_response = self.llm.complete(prompt)
                    response.response = str(llm_response)
                
                logger.info("âœ… æŸ¥è¯¢æ‰§è¡ŒæˆåŠŸï¼ˆæ‰‹åŠ¨è¿‡æ»¤ï¼‰")
                return response
            else:
                # æ‰§è¡ŒæŸ¥è¯¢ï¼ˆè¿‡æ»¤ç”±åå¤„ç†å™¨è‡ªåŠ¨å¤„ç†ï¼‰
                response = self._base_query_engine._query(query_bundle)
                logger.info("âœ… æŸ¥è¯¢æ‰§è¡ŒæˆåŠŸ")
                return response
            
        except IndexError as e:
            if "pop from empty list" in str(e):
                logger.warning(f"âš ï¸ æ£€æµ‹åˆ°å›è°ƒç®¡ç†å™¨æ ˆçŠ¶æ€é—®é¢˜ï¼Œå°è¯•é‡ç½®å›è°ƒç®¡ç†å™¨: {e}")
                # é‡ç½®å›è°ƒç®¡ç†å™¨çŠ¶æ€
                try:
                    from llama_index.core.callbacks import CallbackManager
                    self.callback_manager = CallbackManager()
                    # é‡æ–°åˆ›å»ºåŸºç¡€æŸ¥è¯¢å¼•æ“
                    self._create_base_query_engine()
                    # é‡è¯•æŸ¥è¯¢
                    response = self._base_query_engine._query(query_bundle)
                    logger.info("âœ… æŸ¥è¯¢æ‰§è¡ŒæˆåŠŸï¼ˆé‡è¯•åï¼‰")
                    return response
                except Exception as retry_e:
                    logger.error(f"âŒ é‡è¯•æŸ¥è¯¢å¤±è´¥: {retry_e}")
                    raise e
            else:
                logger.error(f"âŒ æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}")
                raise e
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}")
            raise e
    
    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:
        """
        æ£€ç´¢ç›¸å…³èŠ‚ç‚¹
        
        Args:
            query_bundle: æŸ¥è¯¢åŒ…
            
        Returns:
            æ£€ç´¢åˆ°çš„èŠ‚ç‚¹åˆ—è¡¨
        """
        try:
            logger.info(f"ğŸ” æ£€ç´¢èŠ‚ç‚¹: {query_bundle.query_str[:50]}...")
            
            if self._base_query_engine is None:
                raise RuntimeError("åŸºç¡€æŸ¥è¯¢å¼•æ“æœªåˆå§‹åŒ–")
            
            # ä½¿ç”¨æ£€ç´¢å™¨çš„ retrieve æ–¹æ³•
            if hasattr(self._base_query_engine, '_retriever'):
                nodes = self._base_query_engine._retriever.retrieve(query_bundle)
            else:
                # å¦‚æœæ²¡æœ‰ _retriever å±æ€§ï¼Œå°è¯•ç›´æ¥è°ƒç”¨ retrieve
                nodes = self._base_query_engine.retrieve(query_bundle)
            
            logger.info(f"âœ… æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(nodes)} ä¸ªèŠ‚ç‚¹")
            return nodes
            
        except Exception as e:
            logger.error(f"âŒ èŠ‚ç‚¹æ£€ç´¢å¤±è´¥: {e}")
            raise e
    
    def _retrieve_from_target_files(self, query_bundle: QueryBundle) -> List[NodeWithScore]:
        """
        ä¸“é—¨ä»ç›®æ ‡æ–‡ä»¶ä¸­æ£€ç´¢èŠ‚ç‚¹
        
        Args:
            query_bundle: æŸ¥è¯¢åŒ…
            
        Returns:
            ä»ç›®æ ‡æ–‡ä»¶ä¸­æ£€ç´¢åˆ°çš„èŠ‚ç‚¹åˆ—è¡¨
        """
        try:
            logger.info(f"ğŸ” ä»ç›®æ ‡æ–‡ä»¶æ£€ç´¢èŠ‚ç‚¹: {query_bundle.query_str[:50]}...")
            
            if not self.target_files:
                logger.info("æ— ç›®æ ‡æ–‡ä»¶é™åˆ¶ï¼Œè¿”å›ç©ºåˆ—è¡¨")
                return []
            
            # å¢åŠ æ£€ç´¢æ•°é‡ï¼Œç¡®ä¿èƒ½è·å–åˆ°è¶³å¤Ÿçš„èŠ‚ç‚¹
            increased_top_k = max(self.similarity_top_k * 3, 20)
            
            # ä½¿ç”¨æ£€ç´¢å™¨æ£€ç´¢æ›´å¤šèŠ‚ç‚¹
            if hasattr(self._base_query_engine, '_retriever'):
                nodes = self._base_query_engine._retriever.retrieve(query_bundle)
            else:
                nodes = self._base_query_engine.retrieve(query_bundle)
            
            logger.info(f"ğŸ” æ£€ç´¢åˆ° {len(nodes)} ä¸ªèŠ‚ç‚¹")
            
            # è¿‡æ»¤å‡ºç›®æ ‡æ–‡ä»¶çš„èŠ‚ç‚¹
            target_nodes = self._apply_file_filter(nodes)
            
            # åªè¿”å›å‰ similarity_top_k ä¸ªèŠ‚ç‚¹
            final_nodes = target_nodes[:self.similarity_top_k]
            
            logger.info(f"âœ… ä»ç›®æ ‡æ–‡ä»¶æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(final_nodes)} ä¸ªèŠ‚ç‚¹")
            return final_nodes
            
        except Exception as e:
            logger.error(f"âŒ ä»ç›®æ ‡æ–‡ä»¶æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def _merge_nodes(self, nodes1: List[NodeWithScore], nodes2: List[NodeWithScore]) -> List[NodeWithScore]:
        """
        åˆå¹¶ä¸¤ä¸ªèŠ‚ç‚¹åˆ—è¡¨ï¼Œå»é™¤é‡å¤
        
        Args:
            nodes1: ç¬¬ä¸€ä¸ªèŠ‚ç‚¹åˆ—è¡¨
            nodes2: ç¬¬äºŒä¸ªèŠ‚ç‚¹åˆ—è¡¨
            
        Returns:
            åˆå¹¶åçš„èŠ‚ç‚¹åˆ—è¡¨
        """
        # ä½¿ç”¨èŠ‚ç‚¹IDå»é‡
        seen_ids = set()
        merged_nodes = []
        
        # å…ˆæ·»åŠ ç¬¬ä¸€ä¸ªåˆ—è¡¨çš„èŠ‚ç‚¹
        for node in nodes1:
            node_id = node.node.id_
            if node_id not in seen_ids:
                seen_ids.add(node_id)
                merged_nodes.append(node)
        
        # å†æ·»åŠ ç¬¬äºŒä¸ªåˆ—è¡¨çš„èŠ‚ç‚¹
        for node in nodes2:
            node_id = node.node.id_
            if node_id not in seen_ids:
                seen_ids.add(node_id)
                merged_nodes.append(node)
        
        # æŒ‰åˆ†æ•°æ’åº
        merged_nodes.sort(key=lambda x: x.score, reverse=True)
        
        logger.info(f"åˆå¹¶èŠ‚ç‚¹å®Œæˆï¼ŒèŠ‚ç‚¹1: {len(nodes1)}, èŠ‚ç‚¹2: {len(nodes2)}, åˆå¹¶å: {len(merged_nodes)}")
        return merged_nodes
    
    def _apply_file_filter(self, nodes: List[NodeWithScore]) -> List[NodeWithScore]:
        """
        åº”ç”¨æ–‡ä»¶è¿‡æ»¤
        
        Args:
            nodes: å¾…è¿‡æ»¤çš„èŠ‚ç‚¹åˆ—è¡¨
            
        Returns:
            è¿‡æ»¤åçš„èŠ‚ç‚¹åˆ—è¡¨
        """
        if not self.target_files:
            logger.info("æ— ç›®æ ‡æ–‡ä»¶é™åˆ¶ï¼Œè¿”å›æ‰€æœ‰èŠ‚ç‚¹")
            return nodes
        
        target_files_set = set(self.target_files)
        filtered_nodes = []
        
        for node in nodes:
            if hasattr(node, 'node') and hasattr(node.node, 'metadata'):
                file_name = node.node.metadata.get('file_name', '')
                if file_name in target_files_set:
                    filtered_nodes.append(node)
                    logger.debug(f"ä¿ç•™èŠ‚ç‚¹ï¼Œæ–‡ä»¶: {file_name}")
                else:
                    logger.debug(f"è¿‡æ»¤èŠ‚ç‚¹ï¼Œæ–‡ä»¶: {file_name}")
            else:
                # å¦‚æœæ²¡æœ‰å…ƒæ•°æ®ï¼Œä¹Ÿä¿ç•™èŠ‚ç‚¹ï¼ˆå¯èƒ½æ˜¯ç³»ç»ŸèŠ‚ç‚¹ï¼‰
                filtered_nodes.append(node)
                logger.debug("ä¿ç•™æ— å…ƒæ•°æ®èŠ‚ç‚¹")
        
        logger.info(f"èŠ‚ç‚¹è¿‡æ»¤å®Œæˆï¼ŒåŸå§‹èŠ‚ç‚¹æ•°: {len(nodes)}, è¿‡æ»¤åèŠ‚ç‚¹æ•°: {len(filtered_nodes)}")
        return filtered_nodes
    
    def _extract_keywords_from_target_files(self) -> List[str]:
        """
        ä»ç›®æ ‡æ–‡ä»¶åä¸­æå–å…³é”®è¯
        
        Returns:
            æå–çš„å…³é”®è¯åˆ—è¡¨
        """
        keywords = []
        
        for file_name in self.target_files:
            # ç§»é™¤æ–‡ä»¶æ‰©å±•å
            name_without_ext = file_name.replace('.pdf', '').replace('.md', '').replace('.txt', '')
            
            # ç®€å•çš„å…³é”®è¯æå–é€»è¾‘
            # å¯¹äºä¸­æ–‡æ–‡ä»¶åï¼Œå¯ä»¥æŒ‰å­—ç¬¦åˆ†å‰²æˆ–ä½¿ç”¨å¸¸è§åˆ†éš”ç¬¦
            if 'åŠ' in name_without_ext:
                parts = name_without_ext.split('åŠ')
                keywords.extend([part.strip() for part in parts if part.strip()])
            elif 'ä¸' in name_without_ext:
                parts = name_without_ext.split('ä¸')
                keywords.extend([part.strip() for part in parts if part.strip()])
            elif 'å’Œ' in name_without_ext:
                parts = name_without_ext.split('å’Œ')
                keywords.extend([part.strip() for part in parts if part.strip()])
            else:
                # å¦‚æœæ²¡æœ‰åˆ†éš”ç¬¦ï¼Œå°è¯•æå–å…³é”®è¯
                # å¯¹äº"è½¯ä»¶å·¥ç¨‹åŠDDDå¤§æ¨¡å‹"ï¼Œæå–"è½¯ä»¶å·¥ç¨‹"ã€"DDD"ã€"å¤§æ¨¡å‹"
                if 'è½¯ä»¶å·¥ç¨‹' in name_without_ext:
                    keywords.append('è½¯ä»¶å·¥ç¨‹')
                if 'DDD' in name_without_ext:
                    keywords.append('DDD')
                if 'å¤§æ¨¡å‹' in name_without_ext:
                    keywords.append('å¤§æ¨¡å‹')
                if 'Agent' in name_without_ext:
                    keywords.append('Agent')
                if 'æ¶æ„' in name_without_ext:
                    keywords.append('æ¶æ„')
            
            # æ·»åŠ å®Œæ•´æ–‡ä»¶åä½œä¸ºå…³é”®è¯
            keywords.append(name_without_ext)
        
        # å»é‡å¹¶è¿‡æ»¤
        unique_keywords = list(set(keywords))
        filtered_keywords = [kw for kw in unique_keywords if kw and len(kw) > 1]
        
        logger.info(f"ä»ç›®æ ‡æ–‡ä»¶ {self.target_files} æå–å…³é”®è¯: {filtered_keywords}")
        return filtered_keywords
    
    def get_target_files(self) -> Optional[List[str]]:
        """è·å–ç›®æ ‡æ–‡ä»¶åˆ—è¡¨"""
        return self.target_files
    
    def set_target_files(self, target_files: Optional[List[str]]):
        """è®¾ç½®ç›®æ ‡æ–‡ä»¶åˆ—è¡¨"""
        self.target_files = target_files
        logger.info(f"æ›´æ–°ç›®æ ‡æ–‡ä»¶åˆ—è¡¨: {self.target_files}")
        
        # é‡æ–°åˆ›å»ºåŸºç¡€æŸ¥è¯¢å¼•æ“ä»¥åº”ç”¨æ–°çš„è¿‡æ»¤æ¡ä»¶
        self._create_base_query_engine()
    
    async def _aquery(self, query_bundle: QueryBundle):
        """
        å¼‚æ­¥æŸ¥è¯¢æ–¹æ³•
        
        Args:
            query_bundle: æŸ¥è¯¢åŒ…
            
        Returns:
            æŸ¥è¯¢å“åº”
        """
        # å¯¹äºæˆ‘ä»¬çš„ç”¨ä¾‹ï¼Œç›´æ¥è°ƒç”¨åŒæ­¥æ–¹æ³•
        return self._query(query_bundle)
    
    def _get_prompt_modules(self):
        """
        è·å–æç¤ºæ¨¡å—
        
        Returns:
            æç¤ºæ¨¡å—å­—å…¸
        """
        return {}
